{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./DDE')\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch import nn \n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from dde_config import dde_NN_config\n",
    "from dde_torch import dde_NN, simple_autoencoder_NN, simple_NN, dde_NN_Large_Predictor\n",
    "from stream_dde import supData, unsupData\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dde_nn(data_generator, model_nn):\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    #model_nn.eval()\n",
    "    for i, (v_D, label) in enumerate(data_generator):\n",
    "        recon, code, score, Z_f, z_D = model_nn(v_D.float().cuda())\n",
    "        m = torch.nn.Sigmoid()\n",
    "        logits = torch.squeeze(m(score)).detach().cpu().numpy()\n",
    "        label_ids = label.to('cpu').numpy()\n",
    "        y_label = y_label + label_ids.flatten().tolist()\n",
    "        y_pred = y_pred + logits.flatten().tolist()\n",
    "\n",
    "    return roc_auc_score(y_label, y_pred), y_pred\n",
    "\n",
    "\n",
    "def main_dde_nn(seed):\n",
    "    torch.manual_seed(seed)    # reproducible torch:2 np:3\n",
    "    np.random.seed(seed)\n",
    "    config = dde_NN_config()\n",
    "    pretrain_epoch = config['pretrain_epoch']\n",
    "    pretrain_epoch = 0\n",
    "    train_epoch = 8\n",
    "    lr = config['LR']\n",
    "    thr = config['recon_threshold']\n",
    "    recon_loss_coeff = config['reconstruction_coefficient']\n",
    "    proj_coeff = config['projection_coefficient']\n",
    "    lambda1 = config['lambda1']\n",
    "    lambda2 = config['lambda2']\n",
    "    BATCH_SIZE = config['batch_size']\n",
    "    \n",
    "    loss_r_history = []\n",
    "    loss_p_history = []\n",
    "    loss_c_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    #model_nn = dde_NN_Large_Predictor(**config)\n",
    "    #path = 'model_train_checkpoint_deepDDI_small_EarlyStopping_SemiSup_Full.pt'\n",
    "    path = 'model_pretrain_checkpoint_1.pt'\n",
    "    model_nn = torch.load(path)\n",
    "    \n",
    "    model_nn.cuda()\n",
    "    \n",
    "    #if torch.cuda.device_count() > 1:\n",
    "    #    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    #    model_nn = nn.DataParallel(model_nn)\n",
    "        \n",
    "    opt = torch.optim.Adam(model_nn.parameters(), lr = lr)\n",
    "    \n",
    "    print('--- Data Preparation ---')\n",
    "    \n",
    "    params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 6}\n",
    "\n",
    "    dataFolder = '../../../../../scratch/kh2383/DFI_data'\n",
    "\n",
    "    #df_unsup = pd.read_csv(dataFolder + '/data/unsup_dataset.csv', names = ['idx', 'input1_SMILES', 'input2_SMILES', 'type']).drop(0)# pairs dataframe input1_smiles, input2_smiles\n",
    "    df_ddi = pd.read_csv(dataFolder + '/data/deepDDI_small/fold2/df_ddi_train_val.csv')  # ddi dataframe drug1_smiles, drug2_smiles\n",
    "\n",
    "    #5-fold\n",
    "    kf = KFold(n_splits = 8, shuffle = True, random_state = 1)\n",
    "    #get the 1st fold index\n",
    "    fold_index = next(kf.split(df_ddi), None)\n",
    "    \n",
    "    #ids_unsup = df_unsup.index.values\n",
    "    partition_sup = {'train': fold_index[0], 'val': fold_index[1]}\n",
    "    labels_sup = df_ddi.label.values\n",
    "\n",
    "    #num_of_iter_per_epoch = len(partition_sup['train']) / BATCH_SIZE\n",
    "\n",
    "    #unsup_set = unsupData(ids_unsup, df_unsup)\n",
    "    #unsup_generator = data.DataLoader(unsup_set, **params)\n",
    "\n",
    "    training_set = supData(partition_sup['train'], labels_sup, df_ddi)\n",
    "    training_generator_sup = data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = supData(partition_sup['val'], labels_sup, df_ddi)\n",
    "    validation_generator_sup = data.DataLoader(validation_set, **params)\n",
    "    \n",
    "    max_auc = 0\n",
    "    model_max = copy.deepcopy(model_nn)\n",
    "    \n",
    "    print('--- Pre-training Starts ---')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #len_unsup = len(unsup_generator)\n",
    "    for pre_epo in range(pretrain_epoch):\n",
    "        for i, v_D in enumerate(unsup_generator):\n",
    "            v_D = v_D.float().cuda()\n",
    "            recon, code, score, Z_f, z_D = model_nn(v_D)\n",
    "            loss_r = recon_loss_coeff * F.binary_cross_entropy(recon, v_D.float())\n",
    "            #print(sum(sum(v_D.float())))\n",
    "            #print(torch.norm(z_D - torch.matmul(code, Z_f)))\n",
    "            #loss_p = proj_coeff * (torch.norm(z_D - torch.matmul(code, Z_f)) + lambda1 * torch.sum(torch.abs(code)) / BATCH_SIZE\n",
    "            #                       + lambda2 * torch.norm(Z_f, p='fro')) / BATCH_SIZE\n",
    "            loss_p = proj_coeff * (torch.norm(z_D - torch.matmul(code, Z_f)) + lambda1 * torch.sum(torch.abs(code)) / BATCH_SIZE + lambda2 * torch.norm(Z_f, p='fro') / BATCH_SIZE)\n",
    "            loss = loss_r + loss_p\n",
    "            \n",
    "            loss_r_history.append(loss_r)\n",
    "            loss_p_history.append(loss_p)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            if(i % 10 == 0):\n",
    "                print('Pre-Training at Epoch ' + str(pre_epo) + ' iteration ' + str(i) + ', total loss is '\n",
    "                      + '%.3f' % (loss.cpu().detach().numpy()) + ', proj loss is ' + '%.3f' % (loss_p.cpu().detach().numpy()) \n",
    "                      + ', recon loss is ' + '%.3f' % (loss_r.cpu().detach().numpy()))\n",
    "\n",
    "            if loss_r < thr:\n",
    "                # smaller than certain reconstruction error, -> go to training step\n",
    "                break\n",
    "        \n",
    "            if i == int(len_unsup/4):\n",
    "                torch.save(model_nn, 'model_pretrain_checkpoint_1.pt')\n",
    "            if i == int(len_unsup/2):\n",
    "                torch.save(model_nn, 'model_pretrain_checkpoint_1.pt')\n",
    "        torch.save(model_nn, 'model_nn_pretrain.pt')\n",
    "        \n",
    "    #model_nn = torch.load('model_train_checkpoint_1000_iter_0_epoch.pt')\n",
    "    \n",
    "    print('--- Go for Training ---')\n",
    "    \n",
    "    for tr_epo in range(train_epoch):\n",
    "        for i, (v_D, label) in enumerate(training_generator_sup):\n",
    "            v_D = v_D.float().cuda()\n",
    "            recon, code, score, Z_f, z_D = model_nn(v_D)\n",
    "            \n",
    "            label = Variable(torch.from_numpy(np.array(label)).long())\n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            m = torch.nn.Sigmoid()\n",
    "            n = torch.squeeze(m(score))\n",
    "            \n",
    "            loss_c = loss_fct(n, label.float().cuda())\n",
    "            loss_r = recon_loss_coeff * F.binary_cross_entropy(recon, v_D.float())\n",
    "            # loss_p = proj_coeff * (torch.norm(z_D - torch.matmul(code, Z_f)) + lambda1 * torch.sum(torch.abs(code)) / BATCH_SIZE\n",
    "            #                       + lambda2 * torch.norm(Z_f, p='fro'))\n",
    "            loss_p = proj_coeff * (torch.norm(z_D - torch.matmul(code, Z_f)) + lambda1 * torch.sum(torch.abs(code)) / BATCH_SIZE + lambda2 * torch.norm(Z_f, p='fro') / BATCH_SIZE)\n",
    "            \n",
    "            loss = loss_c + loss_r + loss_p\n",
    "            loss_r_history.append(loss_r)\n",
    "            loss_p_history.append(loss_p)\n",
    "            loss_c_history.append(loss_c)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "                    \n",
    "            if(i % 20 == 0):\n",
    "                print('Training at Epoch ' + str(tr_epo) + ' iteration ' + str(i) + ', total loss is ' + '%.3f' % (loss.cpu().detach().numpy()) + ', proj loss is ' + '%.3f' %(loss_p.cpu().detach().numpy()) + ', recon loss is ' + '%.3f' %(loss_r.cpu().detach().numpy()) + ', classification loss is ' + '%.3f' % (loss_c.cpu().detach().numpy()))\n",
    "            #if(i % 50 ==0):\n",
    "                #print(sum(torch.abs(code[0])))\n",
    "                #print(label)\n",
    "                #print(n)\n",
    "                \n",
    "            #if(i % 1000 == 0):\n",
    "             #   path = 'model_train_checkpoint_' + str(i) + '_iter_' + str(tr_epo)+'_epoch.pt'\n",
    "                #torch.save(model_nn, path)\n",
    "                \n",
    "        with torch.set_grad_enabled(False):\n",
    "            auc, logits = test_dde_nn(validation_generator_sup, model_nn)\n",
    "            if auc > max_auc:\n",
    "                model_max = copy.deepcopy(model_nn)\n",
    "                max_auc = auc\n",
    "                path = '../../../../../scratch/kh2383/DFI_checkpoint/model_train_checkpoint_deepDDI_small_Run2_explainability_seed'+str(seed)+'.pt'\n",
    "                torch.save(model_nn.module.state_dict(), path)    \n",
    "            print('Val at Epoch '+ str(tr_epo) + ' , AUC: '+ str(auc))\n",
    "        \n",
    "    return model_max, loss_c_history, loss_r_history, loss_p_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kh2383/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'dde_torch.dde_NN_Large_Predictor' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation ---\n",
      "--- Pre-training Starts ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 0 iteration 0, total loss is 0.707, proj loss is 0.010, recon loss is 0.005, classification loss is 0.691\n",
      "Training at Epoch 0 iteration 20, total loss is 0.701, proj loss is 0.070, recon loss is 0.005, classification loss is 0.626\n",
      "Training at Epoch 0 iteration 40, total loss is 0.669, proj loss is 0.068, recon loss is 0.005, classification loss is 0.596\n",
      "Training at Epoch 0 iteration 60, total loss is 0.653, proj loss is 0.058, recon loss is 0.005, classification loss is 0.589\n",
      "Training at Epoch 0 iteration 80, total loss is 0.647, proj loss is 0.058, recon loss is 0.005, classification loss is 0.583\n",
      "Training at Epoch 0 iteration 100, total loss is 0.628, proj loss is 0.052, recon loss is 0.005, classification loss is 0.572\n",
      "Training at Epoch 0 iteration 120, total loss is 0.677, proj loss is 0.078, recon loss is 0.005, classification loss is 0.594\n",
      "Training at Epoch 0 iteration 140, total loss is 0.670, proj loss is 0.052, recon loss is 0.005, classification loss is 0.613\n",
      "Training at Epoch 0 iteration 160, total loss is 0.668, proj loss is 0.061, recon loss is 0.005, classification loss is 0.602\n",
      "Training at Epoch 0 iteration 180, total loss is 0.571, proj loss is 0.045, recon loss is 0.005, classification loss is 0.521\n",
      "Training at Epoch 0 iteration 200, total loss is 0.634, proj loss is 0.076, recon loss is 0.005, classification loss is 0.553\n",
      "Training at Epoch 0 iteration 220, total loss is 0.616, proj loss is 0.082, recon loss is 0.005, classification loss is 0.529\n",
      "Training at Epoch 0 iteration 240, total loss is 0.577, proj loss is 0.052, recon loss is 0.005, classification loss is 0.520\n",
      "Val at Epoch 0 , AUC: 0.788240630719\n",
      "Training at Epoch 1 iteration 0, total loss is 0.589, proj loss is 0.066, recon loss is 0.005, classification loss is 0.518\n",
      "Training at Epoch 1 iteration 20, total loss is 0.677, proj loss is 0.055, recon loss is 0.005, classification loss is 0.618\n",
      "Training at Epoch 1 iteration 40, total loss is 0.632, proj loss is 0.110, recon loss is 0.005, classification loss is 0.517\n",
      "Training at Epoch 1 iteration 60, total loss is 0.613, proj loss is 0.061, recon loss is 0.005, classification loss is 0.547\n",
      "Training at Epoch 1 iteration 80, total loss is 0.604, proj loss is 0.083, recon loss is 0.005, classification loss is 0.516\n",
      "Training at Epoch 1 iteration 100, total loss is 0.563, proj loss is 0.052, recon loss is 0.005, classification loss is 0.507\n",
      "Training at Epoch 1 iteration 120, total loss is 0.600, proj loss is 0.075, recon loss is 0.005, classification loss is 0.520\n",
      "Training at Epoch 1 iteration 140, total loss is 0.615, proj loss is 0.059, recon loss is 0.005, classification loss is 0.551\n",
      "Training at Epoch 1 iteration 160, total loss is 0.574, proj loss is 0.059, recon loss is 0.005, classification loss is 0.511\n",
      "Training at Epoch 1 iteration 180, total loss is 0.584, proj loss is 0.084, recon loss is 0.005, classification loss is 0.494\n",
      "Training at Epoch 1 iteration 200, total loss is 0.619, proj loss is 0.068, recon loss is 0.005, classification loss is 0.547\n",
      "Training at Epoch 1 iteration 220, total loss is 0.579, proj loss is 0.058, recon loss is 0.005, classification loss is 0.516\n",
      "Training at Epoch 1 iteration 240, total loss is 0.566, proj loss is 0.064, recon loss is 0.005, classification loss is 0.496\n",
      "Val at Epoch 1 , AUC: 0.810293370858\n",
      "Training at Epoch 2 iteration 0, total loss is 0.558, proj loss is 0.087, recon loss is 0.005, classification loss is 0.466\n",
      "Training at Epoch 2 iteration 20, total loss is 0.579, proj loss is 0.082, recon loss is 0.005, classification loss is 0.492\n",
      "Training at Epoch 2 iteration 40, total loss is 0.633, proj loss is 0.184, recon loss is 0.005, classification loss is 0.444\n",
      "Training at Epoch 2 iteration 60, total loss is 0.596, proj loss is 0.167, recon loss is 0.005, classification loss is 0.424\n",
      "Training at Epoch 2 iteration 80, total loss is 0.663, proj loss is 0.119, recon loss is 0.005, classification loss is 0.539\n",
      "Training at Epoch 2 iteration 100, total loss is 0.599, proj loss is 0.108, recon loss is 0.005, classification loss is 0.487\n",
      "Training at Epoch 2 iteration 120, total loss is 0.561, proj loss is 0.070, recon loss is 0.005, classification loss is 0.485\n",
      "Training at Epoch 2 iteration 140, total loss is 0.584, proj loss is 0.083, recon loss is 0.005, classification loss is 0.496\n",
      "Training at Epoch 2 iteration 160, total loss is 0.556, proj loss is 0.054, recon loss is 0.005, classification loss is 0.497\n",
      "Training at Epoch 2 iteration 180, total loss is 0.548, proj loss is 0.052, recon loss is 0.005, classification loss is 0.491\n",
      "Training at Epoch 2 iteration 200, total loss is 0.599, proj loss is 0.075, recon loss is 0.005, classification loss is 0.519\n",
      "Training at Epoch 2 iteration 220, total loss is 0.502, proj loss is 0.039, recon loss is 0.005, classification loss is 0.457\n",
      "Training at Epoch 2 iteration 240, total loss is 0.526, proj loss is 0.073, recon loss is 0.005, classification loss is 0.448\n",
      "Val at Epoch 2 , AUC: 0.827671226484\n",
      "Training at Epoch 3 iteration 0, total loss is 0.536, proj loss is 0.063, recon loss is 0.005, classification loss is 0.469\n",
      "Training at Epoch 3 iteration 20, total loss is 0.502, proj loss is 0.091, recon loss is 0.005, classification loss is 0.406\n",
      "Training at Epoch 3 iteration 40, total loss is 0.434, proj loss is 0.060, recon loss is 0.005, classification loss is 0.369\n",
      "Training at Epoch 3 iteration 60, total loss is 0.489, proj loss is 0.104, recon loss is 0.005, classification loss is 0.379\n",
      "Training at Epoch 3 iteration 80, total loss is 0.559, proj loss is 0.101, recon loss is 0.005, classification loss is 0.453\n",
      "Training at Epoch 3 iteration 100, total loss is 0.591, proj loss is 0.164, recon loss is 0.005, classification loss is 0.422\n",
      "Training at Epoch 3 iteration 120, total loss is 0.606, proj loss is 0.140, recon loss is 0.005, classification loss is 0.460\n",
      "Training at Epoch 3 iteration 140, total loss is 0.502, proj loss is 0.122, recon loss is 0.005, classification loss is 0.375\n",
      "Training at Epoch 3 iteration 160, total loss is 0.444, proj loss is 0.049, recon loss is 0.005, classification loss is 0.390\n",
      "Training at Epoch 3 iteration 180, total loss is 0.549, proj loss is 0.087, recon loss is 0.005, classification loss is 0.457\n",
      "Training at Epoch 3 iteration 200, total loss is 0.476, proj loss is 0.077, recon loss is 0.005, classification loss is 0.393\n",
      "Training at Epoch 3 iteration 220, total loss is 0.545, proj loss is 0.063, recon loss is 0.005, classification loss is 0.477\n",
      "Training at Epoch 3 iteration 240, total loss is 0.521, proj loss is 0.066, recon loss is 0.005, classification loss is 0.449\n",
      "Val at Epoch 3 , AUC: 0.840423970516\n",
      "Training at Epoch 4 iteration 0, total loss is 0.458, proj loss is 0.068, recon loss is 0.005, classification loss is 0.385\n",
      "Training at Epoch 4 iteration 20, total loss is 0.454, proj loss is 0.111, recon loss is 0.005, classification loss is 0.338\n",
      "Training at Epoch 4 iteration 40, total loss is 0.431, proj loss is 0.102, recon loss is 0.005, classification loss is 0.324\n",
      "Training at Epoch 4 iteration 60, total loss is 0.403, proj loss is 0.045, recon loss is 0.005, classification loss is 0.353\n",
      "Training at Epoch 4 iteration 80, total loss is 0.489, proj loss is 0.113, recon loss is 0.005, classification loss is 0.372\n",
      "Training at Epoch 4 iteration 100, total loss is 0.491, proj loss is 0.093, recon loss is 0.005, classification loss is 0.393\n",
      "Training at Epoch 4 iteration 120, total loss is 0.549, proj loss is 0.083, recon loss is 0.005, classification loss is 0.461\n",
      "Training at Epoch 4 iteration 140, total loss is 0.477, proj loss is 0.074, recon loss is 0.005, classification loss is 0.398\n",
      "Training at Epoch 4 iteration 160, total loss is 0.454, proj loss is 0.043, recon loss is 0.005, classification loss is 0.406\n",
      "Training at Epoch 4 iteration 180, total loss is 0.550, proj loss is 0.110, recon loss is 0.005, classification loss is 0.434\n",
      "Training at Epoch 4 iteration 200, total loss is 0.498, proj loss is 0.114, recon loss is 0.005, classification loss is 0.379\n",
      "Training at Epoch 4 iteration 220, total loss is 0.496, proj loss is 0.094, recon loss is 0.005, classification loss is 0.397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 4 iteration 240, total loss is 0.431, proj loss is 0.036, recon loss is 0.005, classification loss is 0.391\n",
      "Val at Epoch 4 , AUC: 0.849607277911\n",
      "Training at Epoch 5 iteration 0, total loss is 0.395, proj loss is 0.096, recon loss is 0.005, classification loss is 0.293\n",
      "Training at Epoch 5 iteration 20, total loss is 0.308, proj loss is 0.061, recon loss is 0.005, classification loss is 0.242\n",
      "Training at Epoch 5 iteration 40, total loss is 0.464, proj loss is 0.114, recon loss is 0.005, classification loss is 0.345\n",
      "Training at Epoch 5 iteration 60, total loss is 0.499, proj loss is 0.211, recon loss is 0.005, classification loss is 0.283\n",
      "Training at Epoch 5 iteration 80, total loss is 0.387, proj loss is 0.061, recon loss is 0.005, classification loss is 0.322\n",
      "Training at Epoch 5 iteration 100, total loss is 0.443, proj loss is 0.088, recon loss is 0.005, classification loss is 0.351\n",
      "Training at Epoch 5 iteration 120, total loss is 0.410, proj loss is 0.059, recon loss is 0.005, classification loss is 0.346\n",
      "Training at Epoch 5 iteration 140, total loss is 0.450, proj loss is 0.076, recon loss is 0.005, classification loss is 0.369\n",
      "Training at Epoch 5 iteration 160, total loss is 0.366, proj loss is 0.055, recon loss is 0.005, classification loss is 0.306\n",
      "Training at Epoch 5 iteration 180, total loss is 0.472, proj loss is 0.096, recon loss is 0.005, classification loss is 0.371\n",
      "Training at Epoch 5 iteration 200, total loss is 0.388, proj loss is 0.077, recon loss is 0.005, classification loss is 0.306\n",
      "Training at Epoch 5 iteration 220, total loss is 0.505, proj loss is 0.142, recon loss is 0.005, classification loss is 0.357\n",
      "Training at Epoch 5 iteration 240, total loss is 0.485, proj loss is 0.125, recon loss is 0.005, classification loss is 0.355\n",
      "Val at Epoch 5 , AUC: 0.851966346171\n",
      "Training at Epoch 6 iteration 0, total loss is 0.341, proj loss is 0.085, recon loss is 0.005, classification loss is 0.250\n",
      "Training at Epoch 6 iteration 20, total loss is 0.420, proj loss is 0.055, recon loss is 0.005, classification loss is 0.360\n",
      "Training at Epoch 6 iteration 40, total loss is 0.351, proj loss is 0.070, recon loss is 0.005, classification loss is 0.276\n",
      "Training at Epoch 6 iteration 60, total loss is 0.308, proj loss is 0.066, recon loss is 0.005, classification loss is 0.238\n",
      "Training at Epoch 6 iteration 80, total loss is 0.354, proj loss is 0.044, recon loss is 0.005, classification loss is 0.306\n",
      "Training at Epoch 6 iteration 100, total loss is 0.325, proj loss is 0.087, recon loss is 0.005, classification loss is 0.233\n",
      "Training at Epoch 6 iteration 120, total loss is 0.397, proj loss is 0.064, recon loss is 0.005, classification loss is 0.329\n",
      "Training at Epoch 6 iteration 140, total loss is 0.367, proj loss is 0.093, recon loss is 0.005, classification loss is 0.269\n",
      "Training at Epoch 6 iteration 160, total loss is 0.365, proj loss is 0.086, recon loss is 0.005, classification loss is 0.274\n",
      "Training at Epoch 6 iteration 180, total loss is 0.349, proj loss is 0.030, recon loss is 0.005, classification loss is 0.313\n",
      "Training at Epoch 6 iteration 200, total loss is 0.398, proj loss is 0.084, recon loss is 0.005, classification loss is 0.310\n",
      "Training at Epoch 6 iteration 220, total loss is 0.395, proj loss is 0.053, recon loss is 0.005, classification loss is 0.337\n",
      "Training at Epoch 6 iteration 240, total loss is 0.396, proj loss is 0.094, recon loss is 0.005, classification loss is 0.297\n",
      "Val at Epoch 6 , AUC: 0.848316340577\n",
      "Training at Epoch 7 iteration 0, total loss is 0.287, proj loss is 0.065, recon loss is 0.005, classification loss is 0.217\n",
      "Training at Epoch 7 iteration 20, total loss is 0.256, proj loss is 0.074, recon loss is 0.005, classification loss is 0.177\n",
      "Training at Epoch 7 iteration 40, total loss is 0.274, proj loss is 0.068, recon loss is 0.005, classification loss is 0.201\n",
      "Training at Epoch 7 iteration 60, total loss is 0.332, proj loss is 0.113, recon loss is 0.005, classification loss is 0.214\n",
      "Training at Epoch 7 iteration 80, total loss is 0.318, proj loss is 0.056, recon loss is 0.005, classification loss is 0.257\n",
      "Training at Epoch 7 iteration 100, total loss is 0.278, proj loss is 0.066, recon loss is 0.005, classification loss is 0.208\n",
      "Training at Epoch 7 iteration 120, total loss is 0.363, proj loss is 0.090, recon loss is 0.005, classification loss is 0.269\n",
      "Training at Epoch 7 iteration 140, total loss is 0.380, proj loss is 0.074, recon loss is 0.005, classification loss is 0.302\n",
      "Training at Epoch 7 iteration 160, total loss is 0.377, proj loss is 0.087, recon loss is 0.005, classification loss is 0.285\n",
      "Training at Epoch 7 iteration 180, total loss is 0.317, proj loss is 0.038, recon loss is 0.005, classification loss is 0.273\n",
      "Training at Epoch 7 iteration 200, total loss is 0.342, proj loss is 0.049, recon loss is 0.005, classification loss is 0.289\n",
      "Training at Epoch 7 iteration 220, total loss is 0.427, proj loss is 0.146, recon loss is 0.005, classification loss is 0.276\n",
      "Training at Epoch 7 iteration 240, total loss is 0.900, proj loss is 0.671, recon loss is 0.005, classification loss is 0.224\n",
      "Val at Epoch 7 , AUC: 0.853265281815\n",
      "32944.72493267059\n"
     ]
    }
   ],
   "source": [
    "#seed 5\n",
    "s = time()\n",
    "model_max, loss_c, loss_r, loss_p = main_dde_nn(5)\n",
    "e = time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b555af0cdd8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kh2383/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'dde_torch.dde_NN_Large_Predictor' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation ---\n",
      "--- Pre-training Starts ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 0 iteration 0, total loss is 0.710, proj loss is 0.011, recon loss is 0.005, classification loss is 0.694\n",
      "Training at Epoch 0 iteration 20, total loss is 0.713, proj loss is 0.076, recon loss is 0.005, classification loss is 0.632\n",
      "Training at Epoch 0 iteration 40, total loss is 0.686, proj loss is 0.047, recon loss is 0.005, classification loss is 0.634\n",
      "Training at Epoch 0 iteration 60, total loss is 0.704, proj loss is 0.062, recon loss is 0.005, classification loss is 0.636\n",
      "Training at Epoch 0 iteration 80, total loss is 0.668, proj loss is 0.048, recon loss is 0.005, classification loss is 0.614\n",
      "Training at Epoch 0 iteration 100, total loss is 0.642, proj loss is 0.052, recon loss is 0.005, classification loss is 0.585\n",
      "Training at Epoch 0 iteration 120, total loss is 0.651, proj loss is 0.049, recon loss is 0.005, classification loss is 0.597\n",
      "Training at Epoch 0 iteration 140, total loss is 0.631, proj loss is 0.076, recon loss is 0.005, classification loss is 0.550\n",
      "Training at Epoch 0 iteration 160, total loss is 0.608, proj loss is 0.069, recon loss is 0.005, classification loss is 0.534\n",
      "Training at Epoch 0 iteration 180, total loss is 0.599, proj loss is 0.083, recon loss is 0.005, classification loss is 0.511\n",
      "Training at Epoch 0 iteration 200, total loss is 0.621, proj loss is 0.054, recon loss is 0.005, classification loss is 0.562\n",
      "Training at Epoch 0 iteration 220, total loss is 0.589, proj loss is 0.046, recon loss is 0.005, classification loss is 0.538\n",
      "Training at Epoch 0 iteration 240, total loss is 0.597, proj loss is 0.060, recon loss is 0.005, classification loss is 0.533\n",
      "Val at Epoch 0 , AUC: 0.782907974406\n",
      "Training at Epoch 1 iteration 0, total loss is 0.595, proj loss is 0.080, recon loss is 0.005, classification loss is 0.509\n",
      "Training at Epoch 1 iteration 20, total loss is 0.563, proj loss is 0.045, recon loss is 0.005, classification loss is 0.513\n",
      "Training at Epoch 1 iteration 40, total loss is 0.560, proj loss is 0.063, recon loss is 0.005, classification loss is 0.492\n",
      "Training at Epoch 1 iteration 60, total loss is 0.583, proj loss is 0.100, recon loss is 0.005, classification loss is 0.478\n",
      "Training at Epoch 1 iteration 80, total loss is 0.641, proj loss is 0.071, recon loss is 0.005, classification loss is 0.565\n",
      "Training at Epoch 1 iteration 100, total loss is 0.658, proj loss is 0.105, recon loss is 0.005, classification loss is 0.549\n",
      "Training at Epoch 1 iteration 120, total loss is 0.785, proj loss is 0.291, recon loss is 0.005, classification loss is 0.489\n",
      "Training at Epoch 1 iteration 140, total loss is 0.670, proj loss is 0.152, recon loss is 0.005, classification loss is 0.513\n",
      "Training at Epoch 1 iteration 160, total loss is 0.611, proj loss is 0.054, recon loss is 0.005, classification loss is 0.553\n",
      "Training at Epoch 1 iteration 180, total loss is 0.575, proj loss is 0.081, recon loss is 0.005, classification loss is 0.490\n",
      "Training at Epoch 1 iteration 200, total loss is 0.579, proj loss is 0.062, recon loss is 0.005, classification loss is 0.512\n",
      "Training at Epoch 1 iteration 220, total loss is 0.600, proj loss is 0.057, recon loss is 0.005, classification loss is 0.538\n",
      "Training at Epoch 1 iteration 240, total loss is 0.552, proj loss is 0.054, recon loss is 0.005, classification loss is 0.493\n",
      "Val at Epoch 1 , AUC: 0.814736737427\n",
      "Training at Epoch 2 iteration 0, total loss is 0.507, proj loss is 0.079, recon loss is 0.005, classification loss is 0.423\n",
      "Training at Epoch 2 iteration 20, total loss is 0.512, proj loss is 0.043, recon loss is 0.005, classification loss is 0.464\n",
      "Training at Epoch 2 iteration 40, total loss is 0.509, proj loss is 0.067, recon loss is 0.005, classification loss is 0.437\n",
      "Training at Epoch 2 iteration 60, total loss is 0.496, proj loss is 0.060, recon loss is 0.005, classification loss is 0.430\n",
      "Training at Epoch 2 iteration 80, total loss is 0.460, proj loss is 0.058, recon loss is 0.005, classification loss is 0.397\n",
      "Training at Epoch 2 iteration 100, total loss is 0.539, proj loss is 0.072, recon loss is 0.005, classification loss is 0.462\n",
      "Training at Epoch 2 iteration 120, total loss is 0.630, proj loss is 0.141, recon loss is 0.005, classification loss is 0.484\n",
      "Training at Epoch 2 iteration 140, total loss is 0.553, proj loss is 0.062, recon loss is 0.005, classification loss is 0.487\n",
      "Training at Epoch 2 iteration 160, total loss is 0.521, proj loss is 0.074, recon loss is 0.005, classification loss is 0.442\n",
      "Training at Epoch 2 iteration 180, total loss is 0.527, proj loss is 0.046, recon loss is 0.005, classification loss is 0.476\n",
      "Training at Epoch 2 iteration 200, total loss is 0.528, proj loss is 0.083, recon loss is 0.005, classification loss is 0.441\n",
      "Training at Epoch 2 iteration 220, total loss is 0.529, proj loss is 0.031, recon loss is 0.005, classification loss is 0.494\n",
      "Training at Epoch 2 iteration 240, total loss is 0.539, proj loss is 0.038, recon loss is 0.005, classification loss is 0.496\n",
      "Val at Epoch 2 , AUC: 0.833057782233\n",
      "Training at Epoch 3 iteration 0, total loss is 0.476, proj loss is 0.107, recon loss is 0.005, classification loss is 0.364\n",
      "Training at Epoch 3 iteration 20, total loss is 0.682, proj loss is 0.149, recon loss is 0.005, classification loss is 0.527\n",
      "Training at Epoch 3 iteration 40, total loss is 0.522, proj loss is 0.105, recon loss is 0.005, classification loss is 0.411\n",
      "Training at Epoch 3 iteration 60, total loss is 0.473, proj loss is 0.039, recon loss is 0.005, classification loss is 0.429\n",
      "Training at Epoch 3 iteration 80, total loss is 0.536, proj loss is 0.144, recon loss is 0.005, classification loss is 0.388\n",
      "Training at Epoch 3 iteration 100, total loss is 0.559, proj loss is 0.148, recon loss is 0.005, classification loss is 0.406\n",
      "Training at Epoch 3 iteration 120, total loss is 0.602, proj loss is 0.151, recon loss is 0.005, classification loss is 0.446\n",
      "Training at Epoch 3 iteration 140, total loss is 0.455, proj loss is 0.080, recon loss is 0.005, classification loss is 0.371\n",
      "Training at Epoch 3 iteration 160, total loss is 0.478, proj loss is 0.048, recon loss is 0.005, classification loss is 0.424\n",
      "Training at Epoch 3 iteration 180, total loss is 0.478, proj loss is 0.070, recon loss is 0.005, classification loss is 0.404\n",
      "Training at Epoch 3 iteration 200, total loss is 0.559, proj loss is 0.092, recon loss is 0.005, classification loss is 0.462\n",
      "Training at Epoch 3 iteration 220, total loss is 0.472, proj loss is 0.067, recon loss is 0.005, classification loss is 0.399\n",
      "Training at Epoch 3 iteration 240, total loss is 0.474, proj loss is 0.053, recon loss is 0.005, classification loss is 0.416\n",
      "Val at Epoch 3 , AUC: 0.843079638505\n",
      "Training at Epoch 4 iteration 0, total loss is 0.447, proj loss is 0.096, recon loss is 0.005, classification loss is 0.346\n",
      "Training at Epoch 4 iteration 20, total loss is 0.423, proj loss is 0.094, recon loss is 0.005, classification loss is 0.324\n",
      "Training at Epoch 4 iteration 40, total loss is 0.372, proj loss is 0.038, recon loss is 0.005, classification loss is 0.329\n",
      "Training at Epoch 4 iteration 60, total loss is 0.478, proj loss is 0.104, recon loss is 0.005, classification loss is 0.369\n",
      "Training at Epoch 4 iteration 80, total loss is 0.515, proj loss is 0.058, recon loss is 0.005, classification loss is 0.452\n",
      "Training at Epoch 4 iteration 100, total loss is 0.412, proj loss is 0.076, recon loss is 0.005, classification loss is 0.330\n",
      "Training at Epoch 4 iteration 120, total loss is 0.402, proj loss is 0.050, recon loss is 0.005, classification loss is 0.347\n",
      "Training at Epoch 4 iteration 140, total loss is 0.487, proj loss is 0.085, recon loss is 0.005, classification loss is 0.397\n",
      "Training at Epoch 4 iteration 160, total loss is 0.360, proj loss is 0.031, recon loss is 0.005, classification loss is 0.324\n",
      "Training at Epoch 4 iteration 180, total loss is 0.459, proj loss is 0.091, recon loss is 0.005, classification loss is 0.363\n",
      "Training at Epoch 4 iteration 200, total loss is 0.468, proj loss is 0.086, recon loss is 0.005, classification loss is 0.377\n",
      "Training at Epoch 4 iteration 220, total loss is 0.440, proj loss is 0.107, recon loss is 0.005, classification loss is 0.329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 4 iteration 240, total loss is 0.580, proj loss is 0.153, recon loss is 0.005, classification loss is 0.422\n",
      "Val at Epoch 4 , AUC: 0.846897379346\n",
      "Training at Epoch 5 iteration 0, total loss is 0.471, proj loss is 0.189, recon loss is 0.005, classification loss is 0.277\n",
      "Training at Epoch 5 iteration 20, total loss is 0.422, proj loss is 0.122, recon loss is 0.005, classification loss is 0.295\n",
      "Training at Epoch 5 iteration 40, total loss is 0.360, proj loss is 0.101, recon loss is 0.005, classification loss is 0.254\n",
      "Training at Epoch 5 iteration 60, total loss is 0.554, proj loss is 0.199, recon loss is 0.005, classification loss is 0.350\n",
      "Training at Epoch 5 iteration 80, total loss is 0.323, proj loss is 0.037, recon loss is 0.005, classification loss is 0.280\n",
      "Training at Epoch 5 iteration 100, total loss is 0.347, proj loss is 0.047, recon loss is 0.005, classification loss is 0.295\n",
      "Training at Epoch 5 iteration 120, total loss is 0.441, proj loss is 0.089, recon loss is 0.005, classification loss is 0.347\n",
      "Training at Epoch 5 iteration 140, total loss is 0.367, proj loss is 0.062, recon loss is 0.005, classification loss is 0.300\n",
      "Training at Epoch 5 iteration 160, total loss is 0.347, proj loss is 0.065, recon loss is 0.005, classification loss is 0.276\n",
      "Training at Epoch 5 iteration 180, total loss is 0.386, proj loss is 0.063, recon loss is 0.005, classification loss is 0.318\n",
      "Training at Epoch 5 iteration 200, total loss is 0.387, proj loss is 0.092, recon loss is 0.005, classification loss is 0.290\n",
      "Training at Epoch 5 iteration 220, total loss is 0.366, proj loss is 0.051, recon loss is 0.005, classification loss is 0.309\n",
      "Training at Epoch 5 iteration 240, total loss is 0.445, proj loss is 0.155, recon loss is 0.005, classification loss is 0.285\n",
      "Val at Epoch 5 , AUC: 0.849629820917\n",
      "Training at Epoch 6 iteration 0, total loss is 0.498, proj loss is 0.240, recon loss is 0.005, classification loss is 0.252\n",
      "Training at Epoch 6 iteration 20, total loss is 0.258, proj loss is 0.037, recon loss is 0.005, classification loss is 0.216\n",
      "Training at Epoch 6 iteration 40, total loss is 0.461, proj loss is 0.200, recon loss is 0.005, classification loss is 0.256\n",
      "Training at Epoch 6 iteration 60, total loss is 0.356, proj loss is 0.056, recon loss is 0.005, classification loss is 0.295\n",
      "Training at Epoch 6 iteration 80, total loss is 0.571, proj loss is 0.274, recon loss is 0.005, classification loss is 0.292\n",
      "Training at Epoch 6 iteration 100, total loss is 0.393, proj loss is 0.072, recon loss is 0.005, classification loss is 0.317\n",
      "Training at Epoch 6 iteration 120, total loss is 0.362, proj loss is 0.039, recon loss is 0.005, classification loss is 0.318\n",
      "Training at Epoch 6 iteration 140, total loss is 0.402, proj loss is 0.097, recon loss is 0.005, classification loss is 0.300\n",
      "Training at Epoch 6 iteration 160, total loss is 0.409, proj loss is 0.115, recon loss is 0.005, classification loss is 0.290\n",
      "Training at Epoch 6 iteration 180, total loss is 0.371, proj loss is 0.072, recon loss is 0.005, classification loss is 0.295\n",
      "Training at Epoch 6 iteration 200, total loss is 0.412, proj loss is 0.069, recon loss is 0.005, classification loss is 0.338\n",
      "Training at Epoch 6 iteration 220, total loss is 0.437, proj loss is 0.134, recon loss is 0.005, classification loss is 0.299\n",
      "Training at Epoch 6 iteration 240, total loss is 0.509, proj loss is 0.108, recon loss is 0.005, classification loss is 0.396\n",
      "Val at Epoch 6 , AUC: 0.849920383707\n",
      "Training at Epoch 7 iteration 0, total loss is 0.271, proj loss is 0.048, recon loss is 0.005, classification loss is 0.217\n",
      "Training at Epoch 7 iteration 20, total loss is 0.319, proj loss is 0.097, recon loss is 0.005, classification loss is 0.217\n",
      "Training at Epoch 7 iteration 40, total loss is 0.248, proj loss is 0.055, recon loss is 0.005, classification loss is 0.188\n",
      "Training at Epoch 7 iteration 60, total loss is 0.259, proj loss is 0.079, recon loss is 0.005, classification loss is 0.176\n",
      "Training at Epoch 7 iteration 80, total loss is 0.414, proj loss is 0.109, recon loss is 0.005, classification loss is 0.300\n",
      "Training at Epoch 7 iteration 100, total loss is 0.411, proj loss is 0.105, recon loss is 0.005, classification loss is 0.301\n",
      "Training at Epoch 7 iteration 120, total loss is 0.329, proj loss is 0.087, recon loss is 0.005, classification loss is 0.237\n",
      "Training at Epoch 7 iteration 140, total loss is 0.274, proj loss is 0.036, recon loss is 0.005, classification loss is 0.233\n",
      "Training at Epoch 7 iteration 160, total loss is 0.343, proj loss is 0.145, recon loss is 0.005, classification loss is 0.193\n",
      "Training at Epoch 7 iteration 180, total loss is 0.495, proj loss is 0.236, recon loss is 0.005, classification loss is 0.254\n",
      "Training at Epoch 7 iteration 200, total loss is 0.389, proj loss is 0.155, recon loss is 0.005, classification loss is 0.229\n",
      "Training at Epoch 7 iteration 220, total loss is 0.406, proj loss is 0.159, recon loss is 0.005, classification loss is 0.242\n",
      "Training at Epoch 7 iteration 240, total loss is 0.386, proj loss is 0.102, recon loss is 0.005, classification loss is 0.278\n",
      "Val at Epoch 7 , AUC: 0.853733310279\n",
      "32758.405609607697\n"
     ]
    }
   ],
   "source": [
    "# seed 10\n",
    "s = time()\n",
    "model_max, loss_c, loss_r, loss_p = main_dde_nn(10)\n",
    "e = time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kh2383/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'dde_torch.dde_NN_Large_Predictor' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation ---\n",
      "--- Pre-training Starts ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 0 iteration 0, total loss is 0.719, proj loss is 0.010, recon loss is 0.005, classification loss is 0.703\n",
      "Training at Epoch 0 iteration 20, total loss is 0.734, proj loss is 0.073, recon loss is 0.005, classification loss is 0.657\n",
      "Training at Epoch 0 iteration 40, total loss is 0.655, proj loss is 0.050, recon loss is 0.005, classification loss is 0.601\n",
      "Training at Epoch 0 iteration 60, total loss is 0.653, proj loss is 0.043, recon loss is 0.005, classification loss is 0.605\n",
      "Training at Epoch 0 iteration 80, total loss is 0.684, proj loss is 0.046, recon loss is 0.005, classification loss is 0.633\n",
      "Training at Epoch 0 iteration 100, total loss is 0.623, proj loss is 0.050, recon loss is 0.005, classification loss is 0.568\n",
      "Training at Epoch 0 iteration 120, total loss is 0.669, proj loss is 0.053, recon loss is 0.005, classification loss is 0.610\n",
      "Training at Epoch 0 iteration 140, total loss is 0.615, proj loss is 0.059, recon loss is 0.005, classification loss is 0.551\n",
      "Training at Epoch 0 iteration 160, total loss is 0.604, proj loss is 0.054, recon loss is 0.005, classification loss is 0.544\n",
      "Training at Epoch 0 iteration 180, total loss is 0.626, proj loss is 0.057, recon loss is 0.005, classification loss is 0.563\n",
      "Training at Epoch 0 iteration 200, total loss is 0.626, proj loss is 0.071, recon loss is 0.005, classification loss is 0.550\n",
      "Training at Epoch 0 iteration 220, total loss is 0.588, proj loss is 0.057, recon loss is 0.005, classification loss is 0.526\n",
      "Training at Epoch 0 iteration 240, total loss is 0.668, proj loss is 0.049, recon loss is 0.005, classification loss is 0.614\n",
      "Val at Epoch 0 , AUC: 0.786217593346\n",
      "Training at Epoch 1 iteration 0, total loss is 0.624, proj loss is 0.080, recon loss is 0.005, classification loss is 0.539\n",
      "Training at Epoch 1 iteration 20, total loss is 0.638, proj loss is 0.078, recon loss is 0.005, classification loss is 0.555\n",
      "Training at Epoch 1 iteration 40, total loss is 0.554, proj loss is 0.054, recon loss is 0.005, classification loss is 0.495\n",
      "Training at Epoch 1 iteration 60, total loss is 0.604, proj loss is 0.060, recon loss is 0.005, classification loss is 0.538\n",
      "Training at Epoch 1 iteration 80, total loss is 0.570, proj loss is 0.049, recon loss is 0.005, classification loss is 0.516\n",
      "Training at Epoch 1 iteration 100, total loss is 0.565, proj loss is 0.058, recon loss is 0.005, classification loss is 0.502\n",
      "Training at Epoch 1 iteration 120, total loss is 0.582, proj loss is 0.105, recon loss is 0.005, classification loss is 0.472\n",
      "Training at Epoch 1 iteration 140, total loss is 0.633, proj loss is 0.066, recon loss is 0.005, classification loss is 0.562\n",
      "Training at Epoch 1 iteration 160, total loss is 0.620, proj loss is 0.081, recon loss is 0.005, classification loss is 0.534\n",
      "Training at Epoch 1 iteration 180, total loss is 0.615, proj loss is 0.065, recon loss is 0.005, classification loss is 0.545\n",
      "Training at Epoch 1 iteration 200, total loss is 0.555, proj loss is 0.050, recon loss is 0.005, classification loss is 0.500\n",
      "Training at Epoch 1 iteration 220, total loss is 0.571, proj loss is 0.062, recon loss is 0.005, classification loss is 0.504\n",
      "Training at Epoch 1 iteration 240, total loss is 0.608, proj loss is 0.061, recon loss is 0.005, classification loss is 0.542\n",
      "Val at Epoch 1 , AUC: 0.811612276804\n",
      "Training at Epoch 2 iteration 0, total loss is 0.502, proj loss is 0.066, recon loss is 0.005, classification loss is 0.431\n",
      "Training at Epoch 2 iteration 20, total loss is 0.527, proj loss is 0.073, recon loss is 0.005, classification loss is 0.450\n",
      "Training at Epoch 2 iteration 40, total loss is 0.570, proj loss is 0.109, recon loss is 0.005, classification loss is 0.456\n",
      "Training at Epoch 2 iteration 60, total loss is 0.576, proj loss is 0.116, recon loss is 0.005, classification loss is 0.456\n",
      "Training at Epoch 2 iteration 80, total loss is 0.576, proj loss is 0.042, recon loss is 0.005, classification loss is 0.530\n",
      "Training at Epoch 2 iteration 100, total loss is 0.580, proj loss is 0.079, recon loss is 0.005, classification loss is 0.495\n",
      "Training at Epoch 2 iteration 120, total loss is 0.557, proj loss is 0.060, recon loss is 0.005, classification loss is 0.493\n",
      "Training at Epoch 2 iteration 140, total loss is 0.548, proj loss is 0.074, recon loss is 0.005, classification loss is 0.469\n",
      "Training at Epoch 2 iteration 160, total loss is 0.572, proj loss is 0.058, recon loss is 0.005, classification loss is 0.510\n",
      "Training at Epoch 2 iteration 180, total loss is 0.587, proj loss is 0.084, recon loss is 0.005, classification loss is 0.498\n",
      "Training at Epoch 2 iteration 200, total loss is 0.530, proj loss is 0.072, recon loss is 0.005, classification loss is 0.453\n",
      "Training at Epoch 2 iteration 220, total loss is 0.470, proj loss is 0.064, recon loss is 0.005, classification loss is 0.400\n",
      "Training at Epoch 2 iteration 240, total loss is 0.515, proj loss is 0.076, recon loss is 0.005, classification loss is 0.434\n",
      "Val at Epoch 2 , AUC: 0.837547635791\n",
      "Training at Epoch 3 iteration 0, total loss is 0.409, proj loss is 0.063, recon loss is 0.005, classification loss is 0.341\n",
      "Training at Epoch 3 iteration 20, total loss is 0.434, proj loss is 0.062, recon loss is 0.005, classification loss is 0.367\n",
      "Training at Epoch 3 iteration 40, total loss is 0.487, proj loss is 0.064, recon loss is 0.005, classification loss is 0.417\n",
      "Training at Epoch 3 iteration 60, total loss is 0.464, proj loss is 0.086, recon loss is 0.005, classification loss is 0.374\n",
      "Training at Epoch 3 iteration 80, total loss is 0.508, proj loss is 0.047, recon loss is 0.005, classification loss is 0.456\n",
      "Training at Epoch 3 iteration 100, total loss is 0.436, proj loss is 0.060, recon loss is 0.005, classification loss is 0.371\n",
      "Training at Epoch 3 iteration 120, total loss is 0.507, proj loss is 0.083, recon loss is 0.005, classification loss is 0.419\n",
      "Training at Epoch 3 iteration 140, total loss is 0.494, proj loss is 0.029, recon loss is 0.005, classification loss is 0.460\n",
      "Training at Epoch 3 iteration 160, total loss is 0.531, proj loss is 0.075, recon loss is 0.005, classification loss is 0.451\n",
      "Training at Epoch 3 iteration 180, total loss is 0.533, proj loss is 0.106, recon loss is 0.005, classification loss is 0.422\n",
      "Training at Epoch 3 iteration 200, total loss is 0.593, proj loss is 0.165, recon loss is 0.005, classification loss is 0.423\n",
      "Training at Epoch 3 iteration 220, total loss is 0.522, proj loss is 0.135, recon loss is 0.005, classification loss is 0.382\n",
      "Training at Epoch 3 iteration 240, total loss is 0.571, proj loss is 0.081, recon loss is 0.005, classification loss is 0.485\n",
      "Val at Epoch 3 , AUC: 0.84094309646\n",
      "Training at Epoch 4 iteration 0, total loss is 0.504, proj loss is 0.128, recon loss is 0.005, classification loss is 0.371\n",
      "Training at Epoch 4 iteration 20, total loss is 0.504, proj loss is 0.140, recon loss is 0.005, classification loss is 0.358\n",
      "Training at Epoch 4 iteration 40, total loss is 0.495, proj loss is 0.094, recon loss is 0.005, classification loss is 0.396\n",
      "Training at Epoch 4 iteration 60, total loss is 0.407, proj loss is 0.068, recon loss is 0.005, classification loss is 0.334\n",
      "Training at Epoch 4 iteration 80, total loss is 0.506, proj loss is 0.077, recon loss is 0.005, classification loss is 0.424\n",
      "Training at Epoch 4 iteration 100, total loss is 0.379, proj loss is 0.037, recon loss is 0.005, classification loss is 0.337\n",
      "Training at Epoch 4 iteration 120, total loss is 0.515, proj loss is 0.095, recon loss is 0.005, classification loss is 0.415\n",
      "Training at Epoch 4 iteration 140, total loss is 0.403, proj loss is 0.061, recon loss is 0.005, classification loss is 0.337\n",
      "Training at Epoch 4 iteration 160, total loss is 0.403, proj loss is 0.060, recon loss is 0.005, classification loss is 0.338\n",
      "Training at Epoch 4 iteration 180, total loss is 0.488, proj loss is 0.098, recon loss is 0.005, classification loss is 0.385\n",
      "Training at Epoch 4 iteration 200, total loss is 0.391, proj loss is 0.054, recon loss is 0.005, classification loss is 0.332\n",
      "Training at Epoch 4 iteration 220, total loss is 0.430, proj loss is 0.076, recon loss is 0.005, classification loss is 0.349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 4 iteration 240, total loss is 0.481, proj loss is 0.057, recon loss is 0.005, classification loss is 0.419\n",
      "Val at Epoch 4 , AUC: 0.848153317709\n",
      "Training at Epoch 5 iteration 0, total loss is 0.334, proj loss is 0.092, recon loss is 0.005, classification loss is 0.237\n",
      "Training at Epoch 5 iteration 20, total loss is 0.330, proj loss is 0.054, recon loss is 0.005, classification loss is 0.271\n",
      "Training at Epoch 5 iteration 40, total loss is 0.353, proj loss is 0.069, recon loss is 0.005, classification loss is 0.278\n",
      "Training at Epoch 5 iteration 60, total loss is 0.386, proj loss is 0.043, recon loss is 0.005, classification loss is 0.338\n",
      "Training at Epoch 5 iteration 80, total loss is 0.379, proj loss is 0.077, recon loss is 0.005, classification loss is 0.297\n",
      "Training at Epoch 5 iteration 100, total loss is 0.405, proj loss is 0.052, recon loss is 0.005, classification loss is 0.348\n",
      "Training at Epoch 5 iteration 120, total loss is 0.373, proj loss is 0.072, recon loss is 0.005, classification loss is 0.296\n",
      "Training at Epoch 5 iteration 140, total loss is 0.372, proj loss is 0.066, recon loss is 0.005, classification loss is 0.301\n",
      "Training at Epoch 5 iteration 160, total loss is 0.372, proj loss is 0.091, recon loss is 0.005, classification loss is 0.276\n",
      "Training at Epoch 5 iteration 180, total loss is 0.323, proj loss is 0.052, recon loss is 0.005, classification loss is 0.266\n",
      "Training at Epoch 5 iteration 200, total loss is 0.503, proj loss is 0.131, recon loss is 0.005, classification loss is 0.367\n",
      "Training at Epoch 5 iteration 220, total loss is 0.445, proj loss is 0.093, recon loss is 0.005, classification loss is 0.347\n",
      "Training at Epoch 5 iteration 240, total loss is 0.334, proj loss is 0.031, recon loss is 0.005, classification loss is 0.297\n",
      "Val at Epoch 5 , AUC: 0.84867448144\n",
      "Training at Epoch 6 iteration 0, total loss is 0.334, proj loss is 0.091, recon loss is 0.005, classification loss is 0.238\n",
      "Training at Epoch 6 iteration 20, total loss is 0.343, proj loss is 0.130, recon loss is 0.005, classification loss is 0.209\n",
      "Training at Epoch 6 iteration 40, total loss is 0.385, proj loss is 0.151, recon loss is 0.005, classification loss is 0.228\n",
      "Training at Epoch 6 iteration 60, total loss is 0.471, proj loss is 0.216, recon loss is 0.005, classification loss is 0.250\n",
      "Training at Epoch 6 iteration 80, total loss is 0.291, proj loss is 0.032, recon loss is 0.005, classification loss is 0.254\n",
      "Training at Epoch 6 iteration 100, total loss is 0.605, proj loss is 0.343, recon loss is 0.005, classification loss is 0.257\n",
      "Training at Epoch 6 iteration 120, total loss is 0.355, proj loss is 0.071, recon loss is 0.005, classification loss is 0.279\n",
      "Training at Epoch 6 iteration 140, total loss is 0.371, proj loss is 0.093, recon loss is 0.005, classification loss is 0.273\n",
      "Training at Epoch 6 iteration 160, total loss is 0.335, proj loss is 0.063, recon loss is 0.005, classification loss is 0.267\n",
      "Training at Epoch 6 iteration 180, total loss is 0.354, proj loss is 0.082, recon loss is 0.005, classification loss is 0.267\n",
      "Training at Epoch 6 iteration 200, total loss is 0.383, proj loss is 0.061, recon loss is 0.005, classification loss is 0.318\n",
      "Training at Epoch 6 iteration 220, total loss is 0.484, proj loss is 0.064, recon loss is 0.005, classification loss is 0.415\n",
      "Training at Epoch 6 iteration 240, total loss is 0.508, proj loss is 0.127, recon loss is 0.005, classification loss is 0.376\n",
      "Val at Epoch 6 , AUC: 0.849597700318\n",
      "Training at Epoch 7 iteration 0, total loss is 0.349, proj loss is 0.138, recon loss is 0.005, classification loss is 0.206\n",
      "Training at Epoch 7 iteration 20, total loss is 0.354, proj loss is 0.085, recon loss is 0.005, classification loss is 0.264\n",
      "Training at Epoch 7 iteration 40, total loss is 0.265, proj loss is 0.043, recon loss is 0.005, classification loss is 0.216\n",
      "Training at Epoch 7 iteration 60, total loss is 0.319, proj loss is 0.114, recon loss is 0.005, classification loss is 0.200\n",
      "Training at Epoch 7 iteration 80, total loss is 0.279, proj loss is 0.051, recon loss is 0.005, classification loss is 0.224\n",
      "Training at Epoch 7 iteration 100, total loss is 0.338, proj loss is 0.094, recon loss is 0.005, classification loss is 0.239\n",
      "Training at Epoch 7 iteration 120, total loss is 0.359, proj loss is 0.043, recon loss is 0.005, classification loss is 0.311\n",
      "Training at Epoch 7 iteration 140, total loss is 0.360, proj loss is 0.091, recon loss is 0.005, classification loss is 0.264\n",
      "Training at Epoch 7 iteration 160, total loss is 0.322, proj loss is 0.032, recon loss is 0.005, classification loss is 0.284\n",
      "Training at Epoch 7 iteration 180, total loss is 0.331, proj loss is 0.052, recon loss is 0.005, classification loss is 0.273\n",
      "Training at Epoch 7 iteration 200, total loss is 0.292, proj loss is 0.065, recon loss is 0.005, classification loss is 0.222\n",
      "Training at Epoch 7 iteration 220, total loss is 0.349, proj loss is 0.060, recon loss is 0.005, classification loss is 0.284\n",
      "Training at Epoch 7 iteration 240, total loss is 0.400, proj loss is 0.162, recon loss is 0.005, classification loss is 0.233\n",
      "Val at Epoch 7 , AUC: 0.851101229661\n",
      "30364.21092391014\n"
     ]
    }
   ],
   "source": [
    "# seed 15\n",
    "s = time()\n",
    "model_max, loss_c, loss_r, loss_p = main_dde_nn(15)\n",
    "e = time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kh2383/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'dde_torch.dde_NN_Large_Predictor' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pre-training Starts ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 0 iteration 0, total loss is 0.705, proj loss is 0.010, recon loss is 0.005, classification loss is 0.690\n",
      "Training at Epoch 0 iteration 20, total loss is 0.729, proj loss is 0.095, recon loss is 0.005, classification loss is 0.630\n",
      "Training at Epoch 0 iteration 40, total loss is 0.670, proj loss is 0.052, recon loss is 0.005, classification loss is 0.614\n",
      "Training at Epoch 0 iteration 60, total loss is 0.687, proj loss is 0.068, recon loss is 0.005, classification loss is 0.614\n",
      "Training at Epoch 0 iteration 80, total loss is 0.636, proj loss is 0.058, recon loss is 0.005, classification loss is 0.572\n",
      "Training at Epoch 0 iteration 100, total loss is 0.593, proj loss is 0.052, recon loss is 0.005, classification loss is 0.535\n",
      "Training at Epoch 0 iteration 120, total loss is 0.679, proj loss is 0.065, recon loss is 0.005, classification loss is 0.609\n",
      "Training at Epoch 0 iteration 140, total loss is 0.623, proj loss is 0.052, recon loss is 0.005, classification loss is 0.566\n",
      "Training at Epoch 0 iteration 160, total loss is 0.699, proj loss is 0.072, recon loss is 0.005, classification loss is 0.622\n",
      "Training at Epoch 0 iteration 180, total loss is 0.682, proj loss is 0.071, recon loss is 0.005, classification loss is 0.606\n",
      "Training at Epoch 0 iteration 200, total loss is 0.599, proj loss is 0.041, recon loss is 0.005, classification loss is 0.552\n",
      "Training at Epoch 0 iteration 220, total loss is 0.626, proj loss is 0.071, recon loss is 0.005, classification loss is 0.551\n",
      "Training at Epoch 0 iteration 240, total loss is 0.586, proj loss is 0.089, recon loss is 0.005, classification loss is 0.492\n",
      "Val at Epoch 0 , AUC: 0.785956756758\n",
      "Training at Epoch 1 iteration 0, total loss is 0.656, proj loss is 0.074, recon loss is 0.005, classification loss is 0.577\n",
      "Training at Epoch 1 iteration 20, total loss is 0.604, proj loss is 0.074, recon loss is 0.005, classification loss is 0.525\n",
      "Training at Epoch 1 iteration 40, total loss is 0.556, proj loss is 0.057, recon loss is 0.005, classification loss is 0.495\n",
      "Training at Epoch 1 iteration 60, total loss is 0.597, proj loss is 0.055, recon loss is 0.005, classification loss is 0.537\n",
      "Training at Epoch 1 iteration 80, total loss is 0.570, proj loss is 0.048, recon loss is 0.005, classification loss is 0.517\n",
      "Training at Epoch 1 iteration 100, total loss is 0.705, proj loss is 0.133, recon loss is 0.005, classification loss is 0.567\n",
      "Training at Epoch 1 iteration 120, total loss is 0.677, proj loss is 0.162, recon loss is 0.005, classification loss is 0.509\n",
      "Training at Epoch 1 iteration 140, total loss is 0.701, proj loss is 0.181, recon loss is 0.005, classification loss is 0.515\n",
      "Training at Epoch 1 iteration 160, total loss is 0.648, proj loss is 0.120, recon loss is 0.005, classification loss is 0.522\n",
      "Training at Epoch 1 iteration 180, total loss is 0.629, proj loss is 0.128, recon loss is 0.005, classification loss is 0.496\n",
      "Training at Epoch 1 iteration 200, total loss is 0.656, proj loss is 0.091, recon loss is 0.005, classification loss is 0.560\n",
      "Training at Epoch 1 iteration 220, total loss is 0.572, proj loss is 0.034, recon loss is 0.005, classification loss is 0.533\n",
      "Training at Epoch 1 iteration 240, total loss is 0.603, proj loss is 0.047, recon loss is 0.005, classification loss is 0.551\n",
      "Val at Epoch 1 , AUC: 0.815072717369\n",
      "Training at Epoch 2 iteration 0, total loss is 0.555, proj loss is 0.114, recon loss is 0.005, classification loss is 0.436\n",
      "Training at Epoch 2 iteration 20, total loss is 0.533, proj loss is 0.089, recon loss is 0.005, classification loss is 0.439\n",
      "Training at Epoch 2 iteration 40, total loss is 0.495, proj loss is 0.054, recon loss is 0.005, classification loss is 0.436\n",
      "Training at Epoch 2 iteration 60, total loss is 0.520, proj loss is 0.057, recon loss is 0.005, classification loss is 0.458\n",
      "Training at Epoch 2 iteration 80, total loss is 0.519, proj loss is 0.086, recon loss is 0.005, classification loss is 0.428\n",
      "Training at Epoch 2 iteration 100, total loss is 0.509, proj loss is 0.064, recon loss is 0.005, classification loss is 0.440\n",
      "Training at Epoch 2 iteration 120, total loss is 0.590, proj loss is 0.049, recon loss is 0.005, classification loss is 0.536\n",
      "Training at Epoch 2 iteration 140, total loss is 0.498, proj loss is 0.063, recon loss is 0.005, classification loss is 0.431\n",
      "Training at Epoch 2 iteration 160, total loss is 0.650, proj loss is 0.054, recon loss is 0.005, classification loss is 0.591\n",
      "Training at Epoch 2 iteration 180, total loss is 0.501, proj loss is 0.055, recon loss is 0.005, classification loss is 0.440\n",
      "Training at Epoch 2 iteration 200, total loss is 0.584, proj loss is 0.061, recon loss is 0.005, classification loss is 0.518\n",
      "Training at Epoch 2 iteration 220, total loss is 0.522, proj loss is 0.061, recon loss is 0.005, classification loss is 0.456\n",
      "Training at Epoch 2 iteration 240, total loss is 0.577, proj loss is 0.066, recon loss is 0.005, classification loss is 0.506\n",
      "Val at Epoch 2 , AUC: 0.829551542431\n",
      "Training at Epoch 3 iteration 0, total loss is 0.478, proj loss is 0.060, recon loss is 0.005, classification loss is 0.413\n",
      "Training at Epoch 3 iteration 20, total loss is 0.449, proj loss is 0.059, recon loss is 0.005, classification loss is 0.385\n",
      "Training at Epoch 3 iteration 40, total loss is 0.460, proj loss is 0.054, recon loss is 0.005, classification loss is 0.401\n",
      "Training at Epoch 3 iteration 60, total loss is 0.566, proj loss is 0.100, recon loss is 0.005, classification loss is 0.461\n",
      "Training at Epoch 3 iteration 80, total loss is 0.509, proj loss is 0.051, recon loss is 0.005, classification loss is 0.453\n",
      "Training at Epoch 3 iteration 100, total loss is 0.500, proj loss is 0.090, recon loss is 0.005, classification loss is 0.405\n",
      "Training at Epoch 3 iteration 120, total loss is 0.508, proj loss is 0.058, recon loss is 0.005, classification loss is 0.444\n",
      "Training at Epoch 3 iteration 140, total loss is 0.504, proj loss is 0.076, recon loss is 0.005, classification loss is 0.422\n",
      "Training at Epoch 3 iteration 160, total loss is 0.496, proj loss is 0.074, recon loss is 0.005, classification loss is 0.417\n",
      "Training at Epoch 3 iteration 180, total loss is 0.498, proj loss is 0.040, recon loss is 0.005, classification loss is 0.453\n",
      "Training at Epoch 3 iteration 200, total loss is 0.549, proj loss is 0.088, recon loss is 0.005, classification loss is 0.457\n",
      "Training at Epoch 3 iteration 220, total loss is 0.459, proj loss is 0.047, recon loss is 0.005, classification loss is 0.407\n",
      "Training at Epoch 3 iteration 240, total loss is 0.544, proj loss is 0.105, recon loss is 0.005, classification loss is 0.434\n",
      "Val at Epoch 3 , AUC: 0.830305395833\n",
      "Training at Epoch 4 iteration 0, total loss is 0.407, proj loss is 0.058, recon loss is 0.005, classification loss is 0.344\n",
      "Training at Epoch 4 iteration 20, total loss is 0.474, proj loss is 0.098, recon loss is 0.005, classification loss is 0.371\n",
      "Training at Epoch 4 iteration 40, total loss is 0.458, proj loss is 0.060, recon loss is 0.005, classification loss is 0.393\n",
      "Training at Epoch 4 iteration 60, total loss is 0.446, proj loss is 0.039, recon loss is 0.005, classification loss is 0.402\n",
      "Training at Epoch 4 iteration 80, total loss is 0.439, proj loss is 0.060, recon loss is 0.005, classification loss is 0.373\n",
      "Training at Epoch 4 iteration 100, total loss is 0.515, proj loss is 0.110, recon loss is 0.005, classification loss is 0.400\n",
      "Training at Epoch 4 iteration 120, total loss is 0.523, proj loss is 0.113, recon loss is 0.005, classification loss is 0.405\n",
      "Training at Epoch 4 iteration 140, total loss is 0.424, proj loss is 0.044, recon loss is 0.005, classification loss is 0.375\n",
      "Training at Epoch 4 iteration 160, total loss is 0.449, proj loss is 0.054, recon loss is 0.005, classification loss is 0.390\n",
      "Training at Epoch 4 iteration 180, total loss is 0.445, proj loss is 0.062, recon loss is 0.005, classification loss is 0.377\n",
      "Training at Epoch 4 iteration 200, total loss is 0.466, proj loss is 0.054, recon loss is 0.005, classification loss is 0.407\n",
      "Training at Epoch 4 iteration 220, total loss is 0.476, proj loss is 0.090, recon loss is 0.005, classification loss is 0.381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 4 iteration 240, total loss is 0.483, proj loss is 0.047, recon loss is 0.005, classification loss is 0.431\n",
      "Val at Epoch 4 , AUC: 0.847978781351\n",
      "Training at Epoch 5 iteration 0, total loss is 0.362, proj loss is 0.089, recon loss is 0.005, classification loss is 0.268\n",
      "Training at Epoch 5 iteration 20, total loss is 0.381, proj loss is 0.076, recon loss is 0.005, classification loss is 0.300\n",
      "Training at Epoch 5 iteration 40, total loss is 0.408, proj loss is 0.060, recon loss is 0.005, classification loss is 0.342\n",
      "Training at Epoch 5 iteration 60, total loss is 0.358, proj loss is 0.085, recon loss is 0.005, classification loss is 0.268\n",
      "Training at Epoch 5 iteration 80, total loss is 0.358, proj loss is 0.057, recon loss is 0.005, classification loss is 0.296\n",
      "Training at Epoch 5 iteration 100, total loss is 0.412, proj loss is 0.056, recon loss is 0.005, classification loss is 0.350\n",
      "Training at Epoch 5 iteration 120, total loss is 0.377, proj loss is 0.072, recon loss is 0.005, classification loss is 0.301\n",
      "Training at Epoch 5 iteration 140, total loss is 0.401, proj loss is 0.066, recon loss is 0.005, classification loss is 0.330\n",
      "Training at Epoch 5 iteration 160, total loss is 0.484, proj loss is 0.051, recon loss is 0.005, classification loss is 0.428\n",
      "Training at Epoch 5 iteration 180, total loss is 0.514, proj loss is 0.113, recon loss is 0.005, classification loss is 0.395\n",
      "Training at Epoch 5 iteration 200, total loss is 0.426, proj loss is 0.088, recon loss is 0.005, classification loss is 0.332\n",
      "Training at Epoch 5 iteration 220, total loss is 0.426, proj loss is 0.054, recon loss is 0.005, classification loss is 0.366\n",
      "Training at Epoch 5 iteration 240, total loss is 0.478, proj loss is 0.149, recon loss is 0.005, classification loss is 0.324\n",
      "Val at Epoch 5 , AUC: 0.850231655495\n",
      "Training at Epoch 6 iteration 0, total loss is 0.494, proj loss is 0.235, recon loss is 0.005, classification loss is 0.254\n",
      "Training at Epoch 6 iteration 20, total loss is 0.609, proj loss is 0.327, recon loss is 0.005, classification loss is 0.276\n",
      "Training at Epoch 6 iteration 40, total loss is 0.358, proj loss is 0.113, recon loss is 0.005, classification loss is 0.240\n",
      "Training at Epoch 6 iteration 60, total loss is 0.728, proj loss is 0.438, recon loss is 0.005, classification loss is 0.286\n",
      "Training at Epoch 6 iteration 80, total loss is 0.377, proj loss is 0.072, recon loss is 0.005, classification loss is 0.300\n",
      "Training at Epoch 6 iteration 100, total loss is 0.361, proj loss is 0.062, recon loss is 0.005, classification loss is 0.294\n",
      "Training at Epoch 6 iteration 120, total loss is 0.391, proj loss is 0.072, recon loss is 0.005, classification loss is 0.315\n",
      "Training at Epoch 6 iteration 140, total loss is 0.387, proj loss is 0.066, recon loss is 0.005, classification loss is 0.316\n",
      "Training at Epoch 6 iteration 160, total loss is 0.324, proj loss is 0.069, recon loss is 0.005, classification loss is 0.250\n",
      "Training at Epoch 6 iteration 180, total loss is 0.330, proj loss is 0.064, recon loss is 0.005, classification loss is 0.261\n",
      "Training at Epoch 6 iteration 200, total loss is 0.405, proj loss is 0.046, recon loss is 0.005, classification loss is 0.355\n",
      "Training at Epoch 6 iteration 220, total loss is 0.347, proj loss is 0.071, recon loss is 0.005, classification loss is 0.271\n",
      "Training at Epoch 6 iteration 240, total loss is 0.354, proj loss is 0.057, recon loss is 0.005, classification loss is 0.292\n",
      "Val at Epoch 6 , AUC: 0.851081106526\n",
      "Training at Epoch 7 iteration 0, total loss is 0.327, proj loss is 0.088, recon loss is 0.005, classification loss is 0.234\n",
      "Training at Epoch 7 iteration 20, total loss is 0.288, proj loss is 0.079, recon loss is 0.005, classification loss is 0.204\n",
      "Training at Epoch 7 iteration 40, total loss is 0.294, proj loss is 0.046, recon loss is 0.005, classification loss is 0.244\n",
      "Training at Epoch 7 iteration 60, total loss is 0.336, proj loss is 0.069, recon loss is 0.005, classification loss is 0.262\n",
      "Training at Epoch 7 iteration 80, total loss is 0.333, proj loss is 0.079, recon loss is 0.005, classification loss is 0.249\n",
      "Training at Epoch 7 iteration 100, total loss is 0.276, proj loss is 0.079, recon loss is 0.005, classification loss is 0.193\n",
      "Training at Epoch 7 iteration 120, total loss is 0.329, proj loss is 0.071, recon loss is 0.005, classification loss is 0.254\n",
      "Training at Epoch 7 iteration 140, total loss is 0.360, proj loss is 0.077, recon loss is 0.005, classification loss is 0.278\n",
      "Training at Epoch 7 iteration 160, total loss is 0.312, proj loss is 0.063, recon loss is 0.005, classification loss is 0.244\n",
      "Training at Epoch 7 iteration 180, total loss is 0.350, proj loss is 0.075, recon loss is 0.005, classification loss is 0.270\n",
      "Training at Epoch 7 iteration 200, total loss is 0.329, proj loss is 0.073, recon loss is 0.005, classification loss is 0.251\n",
      "Training at Epoch 7 iteration 220, total loss is 0.323, proj loss is 0.051, recon loss is 0.005, classification loss is 0.267\n",
      "Training at Epoch 7 iteration 240, total loss is 0.369, proj loss is 0.067, recon loss is 0.005, classification loss is 0.297\n",
      "Val at Epoch 7 , AUC: 0.855362749315\n",
      "30479.096859931946\n"
     ]
    }
   ],
   "source": [
    "# seed 15\n",
    "s = time()\n",
    "model_max, loss_c, loss_r, loss_p = main_dde_nn(20)\n",
    "e = time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kh2383/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'dde_torch.dde_NN_Large_Predictor' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pre-training Starts ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 0 iteration 0, total loss is 0.713, proj loss is 0.010, recon loss is 0.005, classification loss is 0.698\n",
      "Training at Epoch 0 iteration 20, total loss is 0.736, proj loss is 0.074, recon loss is 0.005, classification loss is 0.657\n",
      "Training at Epoch 0 iteration 40, total loss is 0.668, proj loss is 0.058, recon loss is 0.005, classification loss is 0.604\n",
      "Training at Epoch 0 iteration 60, total loss is 0.645, proj loss is 0.050, recon loss is 0.005, classification loss is 0.590\n",
      "Training at Epoch 0 iteration 80, total loss is 0.666, proj loss is 0.055, recon loss is 0.005, classification loss is 0.606\n",
      "Training at Epoch 0 iteration 100, total loss is 0.653, proj loss is 0.061, recon loss is 0.005, classification loss is 0.587\n",
      "Training at Epoch 0 iteration 120, total loss is 0.656, proj loss is 0.077, recon loss is 0.005, classification loss is 0.574\n",
      "Training at Epoch 0 iteration 140, total loss is 0.640, proj loss is 0.049, recon loss is 0.005, classification loss is 0.586\n",
      "Training at Epoch 0 iteration 160, total loss is 0.655, proj loss is 0.053, recon loss is 0.005, classification loss is 0.597\n",
      "Training at Epoch 0 iteration 180, total loss is 0.613, proj loss is 0.036, recon loss is 0.005, classification loss is 0.573\n",
      "Training at Epoch 0 iteration 200, total loss is 0.628, proj loss is 0.077, recon loss is 0.005, classification loss is 0.547\n",
      "Training at Epoch 0 iteration 220, total loss is 0.627, proj loss is 0.047, recon loss is 0.005, classification loss is 0.574\n",
      "Training at Epoch 0 iteration 240, total loss is 0.586, proj loss is 0.049, recon loss is 0.005, classification loss is 0.532\n",
      "Val at Epoch 0 , AUC: 0.78092536161\n",
      "Training at Epoch 1 iteration 0, total loss is 0.592, proj loss is 0.062, recon loss is 0.005, classification loss is 0.525\n",
      "Training at Epoch 1 iteration 20, total loss is 0.667, proj loss is 0.113, recon loss is 0.005, classification loss is 0.549\n",
      "Training at Epoch 1 iteration 40, total loss is 0.588, proj loss is 0.087, recon loss is 0.005, classification loss is 0.496\n",
      "Training at Epoch 1 iteration 60, total loss is 0.606, proj loss is 0.064, recon loss is 0.005, classification loss is 0.537\n",
      "Training at Epoch 1 iteration 80, total loss is 0.586, proj loss is 0.065, recon loss is 0.005, classification loss is 0.517\n",
      "Training at Epoch 1 iteration 100, total loss is 0.602, proj loss is 0.049, recon loss is 0.005, classification loss is 0.549\n",
      "Training at Epoch 1 iteration 120, total loss is 0.556, proj loss is 0.057, recon loss is 0.005, classification loss is 0.494\n",
      "Training at Epoch 1 iteration 140, total loss is 0.590, proj loss is 0.079, recon loss is 0.005, classification loss is 0.507\n",
      "Training at Epoch 1 iteration 160, total loss is 0.562, proj loss is 0.056, recon loss is 0.005, classification loss is 0.501\n",
      "Training at Epoch 1 iteration 180, total loss is 0.581, proj loss is 0.082, recon loss is 0.005, classification loss is 0.494\n",
      "Training at Epoch 1 iteration 200, total loss is 0.552, proj loss is 0.052, recon loss is 0.005, classification loss is 0.495\n",
      "Training at Epoch 1 iteration 220, total loss is 0.598, proj loss is 0.054, recon loss is 0.005, classification loss is 0.539\n",
      "Training at Epoch 1 iteration 240, total loss is 0.504, proj loss is 0.063, recon loss is 0.005, classification loss is 0.435\n",
      "Val at Epoch 1 , AUC: 0.816763544704\n",
      "Training at Epoch 2 iteration 0, total loss is 0.520, proj loss is 0.057, recon loss is 0.005, classification loss is 0.459\n",
      "Training at Epoch 2 iteration 20, total loss is 0.578, proj loss is 0.184, recon loss is 0.005, classification loss is 0.389\n",
      "Training at Epoch 2 iteration 40, total loss is 0.566, proj loss is 0.114, recon loss is 0.005, classification loss is 0.448\n",
      "Training at Epoch 2 iteration 60, total loss is 0.544, proj loss is 0.049, recon loss is 0.005, classification loss is 0.489\n",
      "Training at Epoch 2 iteration 80, total loss is 0.572, proj loss is 0.100, recon loss is 0.005, classification loss is 0.467\n",
      "Training at Epoch 2 iteration 100, total loss is 0.523, proj loss is 0.038, recon loss is 0.005, classification loss is 0.481\n",
      "Training at Epoch 2 iteration 120, total loss is 0.555, proj loss is 0.064, recon loss is 0.005, classification loss is 0.486\n",
      "Training at Epoch 2 iteration 140, total loss is 0.510, proj loss is 0.070, recon loss is 0.005, classification loss is 0.435\n",
      "Training at Epoch 2 iteration 160, total loss is 0.556, proj loss is 0.077, recon loss is 0.005, classification loss is 0.474\n",
      "Training at Epoch 2 iteration 180, total loss is 0.517, proj loss is 0.065, recon loss is 0.005, classification loss is 0.446\n",
      "Training at Epoch 2 iteration 200, total loss is 0.529, proj loss is 0.069, recon loss is 0.005, classification loss is 0.455\n",
      "Training at Epoch 2 iteration 220, total loss is 0.566, proj loss is 0.123, recon loss is 0.005, classification loss is 0.437\n",
      "Training at Epoch 2 iteration 240, total loss is 0.596, proj loss is 0.142, recon loss is 0.005, classification loss is 0.449\n",
      "Val at Epoch 2 , AUC: 0.832194728982\n",
      "Training at Epoch 3 iteration 0, total loss is 0.478, proj loss is 0.098, recon loss is 0.005, classification loss is 0.375\n",
      "Training at Epoch 3 iteration 20, total loss is 0.463, proj loss is 0.079, recon loss is 0.005, classification loss is 0.380\n",
      "Training at Epoch 3 iteration 40, total loss is 0.435, proj loss is 0.102, recon loss is 0.005, classification loss is 0.328\n",
      "Training at Epoch 3 iteration 60, total loss is 0.431, proj loss is 0.035, recon loss is 0.005, classification loss is 0.391\n",
      "Training at Epoch 3 iteration 80, total loss is 0.550, proj loss is 0.158, recon loss is 0.005, classification loss is 0.387\n",
      "Training at Epoch 3 iteration 100, total loss is 0.479, proj loss is 0.097, recon loss is 0.005, classification loss is 0.376\n",
      "Training at Epoch 3 iteration 120, total loss is 0.464, proj loss is 0.043, recon loss is 0.005, classification loss is 0.416\n",
      "Training at Epoch 3 iteration 140, total loss is 0.546, proj loss is 0.100, recon loss is 0.005, classification loss is 0.441\n",
      "Training at Epoch 3 iteration 160, total loss is 0.502, proj loss is 0.101, recon loss is 0.005, classification loss is 0.396\n",
      "Training at Epoch 3 iteration 180, total loss is 0.414, proj loss is 0.036, recon loss is 0.005, classification loss is 0.373\n",
      "Training at Epoch 3 iteration 200, total loss is 0.447, proj loss is 0.062, recon loss is 0.005, classification loss is 0.379\n",
      "Training at Epoch 3 iteration 220, total loss is 0.515, proj loss is 0.100, recon loss is 0.005, classification loss is 0.410\n",
      "Training at Epoch 3 iteration 240, total loss is 0.494, proj loss is 0.077, recon loss is 0.005, classification loss is 0.413\n",
      "Val at Epoch 3 , AUC: 0.850337034496\n",
      "Training at Epoch 4 iteration 0, total loss is 0.485, proj loss is 0.123, recon loss is 0.005, classification loss is 0.357\n",
      "Training at Epoch 4 iteration 20, total loss is 0.553, proj loss is 0.159, recon loss is 0.005, classification loss is 0.389\n",
      "Training at Epoch 4 iteration 40, total loss is 0.403, proj loss is 0.101, recon loss is 0.005, classification loss is 0.297\n",
      "Training at Epoch 4 iteration 60, total loss is 0.436, proj loss is 0.048, recon loss is 0.005, classification loss is 0.383\n",
      "Training at Epoch 4 iteration 80, total loss is 0.471, proj loss is 0.062, recon loss is 0.005, classification loss is 0.404\n",
      "Training at Epoch 4 iteration 100, total loss is 0.429, proj loss is 0.060, recon loss is 0.005, classification loss is 0.364\n",
      "Training at Epoch 4 iteration 120, total loss is 0.548, proj loss is 0.066, recon loss is 0.005, classification loss is 0.478\n",
      "Training at Epoch 4 iteration 140, total loss is 0.388, proj loss is 0.036, recon loss is 0.005, classification loss is 0.347\n",
      "Training at Epoch 4 iteration 160, total loss is 0.472, proj loss is 0.080, recon loss is 0.005, classification loss is 0.388\n",
      "Training at Epoch 4 iteration 180, total loss is 0.423, proj loss is 0.043, recon loss is 0.005, classification loss is 0.376\n",
      "Training at Epoch 4 iteration 200, total loss is 0.484, proj loss is 0.079, recon loss is 0.005, classification loss is 0.400\n",
      "Training at Epoch 4 iteration 220, total loss is 0.478, proj loss is 0.161, recon loss is 0.005, classification loss is 0.311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 4 iteration 240, total loss is 0.389, proj loss is 0.073, recon loss is 0.005, classification loss is 0.312\n",
      "Val at Epoch 4 , AUC: 0.847378653419\n",
      "Training at Epoch 5 iteration 0, total loss is 0.402, proj loss is 0.103, recon loss is 0.005, classification loss is 0.294\n",
      "Training at Epoch 5 iteration 20, total loss is 0.581, proj loss is 0.102, recon loss is 0.005, classification loss is 0.474\n",
      "Training at Epoch 5 iteration 40, total loss is 0.468, proj loss is 0.195, recon loss is 0.005, classification loss is 0.268\n",
      "Training at Epoch 5 iteration 60, total loss is 0.582, proj loss is 0.276, recon loss is 0.005, classification loss is 0.301\n",
      "Training at Epoch 5 iteration 80, total loss is 0.430, proj loss is 0.042, recon loss is 0.005, classification loss is 0.383\n",
      "Training at Epoch 5 iteration 100, total loss is 0.482, proj loss is 0.103, recon loss is 0.005, classification loss is 0.374\n",
      "Training at Epoch 5 iteration 120, total loss is 0.387, proj loss is 0.095, recon loss is 0.005, classification loss is 0.287\n",
      "Training at Epoch 5 iteration 140, total loss is 0.353, proj loss is 0.072, recon loss is 0.005, classification loss is 0.276\n",
      "Training at Epoch 5 iteration 160, total loss is 0.468, proj loss is 0.101, recon loss is 0.005, classification loss is 0.362\n",
      "Training at Epoch 5 iteration 180, total loss is 0.385, proj loss is 0.028, recon loss is 0.005, classification loss is 0.352\n",
      "Training at Epoch 5 iteration 200, total loss is 0.398, proj loss is 0.058, recon loss is 0.005, classification loss is 0.334\n",
      "Training at Epoch 5 iteration 220, total loss is 0.410, proj loss is 0.100, recon loss is 0.005, classification loss is 0.306\n",
      "Training at Epoch 5 iteration 240, total loss is 0.406, proj loss is 0.110, recon loss is 0.005, classification loss is 0.291\n",
      "Val at Epoch 5 , AUC: 0.8523455017\n",
      "Training at Epoch 6 iteration 0, total loss is 0.346, proj loss is 0.056, recon loss is 0.005, classification loss is 0.285\n",
      "Training at Epoch 6 iteration 20, total loss is 0.370, proj loss is 0.095, recon loss is 0.005, classification loss is 0.270\n",
      "Training at Epoch 6 iteration 40, total loss is 0.360, proj loss is 0.126, recon loss is 0.005, classification loss is 0.228\n",
      "Training at Epoch 6 iteration 60, total loss is 0.386, proj loss is 0.155, recon loss is 0.005, classification loss is 0.226\n",
      "Training at Epoch 6 iteration 80, total loss is 0.371, proj loss is 0.132, recon loss is 0.005, classification loss is 0.234\n",
      "Training at Epoch 6 iteration 100, total loss is 0.382, proj loss is 0.120, recon loss is 0.005, classification loss is 0.257\n",
      "Training at Epoch 6 iteration 120, total loss is 0.370, proj loss is 0.087, recon loss is 0.005, classification loss is 0.278\n",
      "Training at Epoch 6 iteration 140, total loss is 0.305, proj loss is 0.025, recon loss is 0.005, classification loss is 0.275\n",
      "Training at Epoch 6 iteration 160, total loss is 0.585, proj loss is 0.251, recon loss is 0.005, classification loss is 0.329\n",
      "Training at Epoch 6 iteration 180, total loss is 0.359, proj loss is 0.101, recon loss is 0.005, classification loss is 0.253\n",
      "Training at Epoch 6 iteration 200, total loss is 0.404, proj loss is 0.149, recon loss is 0.005, classification loss is 0.250\n",
      "Training at Epoch 6 iteration 220, total loss is 0.419, proj loss is 0.090, recon loss is 0.005, classification loss is 0.325\n",
      "Training at Epoch 6 iteration 240, total loss is 0.384, proj loss is 0.083, recon loss is 0.005, classification loss is 0.296\n",
      "Val at Epoch 6 , AUC: 0.853099737187\n",
      "Training at Epoch 7 iteration 0, total loss is 0.264, proj loss is 0.045, recon loss is 0.005, classification loss is 0.213\n",
      "Training at Epoch 7 iteration 20, total loss is 0.314, proj loss is 0.134, recon loss is 0.005, classification loss is 0.175\n",
      "Training at Epoch 7 iteration 40, total loss is 0.246, proj loss is 0.048, recon loss is 0.005, classification loss is 0.193\n",
      "Training at Epoch 7 iteration 60, total loss is 0.357, proj loss is 0.085, recon loss is 0.005, classification loss is 0.267\n",
      "Training at Epoch 7 iteration 80, total loss is 0.403, proj loss is 0.076, recon loss is 0.005, classification loss is 0.322\n",
      "Training at Epoch 7 iteration 100, total loss is 0.289, proj loss is 0.030, recon loss is 0.005, classification loss is 0.254\n",
      "Training at Epoch 7 iteration 120, total loss is 0.436, proj loss is 0.219, recon loss is 0.005, classification loss is 0.212\n",
      "Training at Epoch 7 iteration 140, total loss is 0.410, proj loss is 0.100, recon loss is 0.005, classification loss is 0.305\n",
      "Training at Epoch 7 iteration 160, total loss is 0.326, proj loss is 0.067, recon loss is 0.005, classification loss is 0.254\n",
      "Training at Epoch 7 iteration 180, total loss is 0.309, proj loss is 0.047, recon loss is 0.005, classification loss is 0.257\n",
      "Training at Epoch 7 iteration 200, total loss is 0.388, proj loss is 0.124, recon loss is 0.005, classification loss is 0.259\n",
      "Training at Epoch 7 iteration 220, total loss is 0.297, proj loss is 0.053, recon loss is 0.005, classification loss is 0.239\n",
      "Training at Epoch 7 iteration 240, total loss is 0.351, proj loss is 0.082, recon loss is 0.005, classification loss is 0.264\n",
      "Val at Epoch 7 , AUC: 0.849475203916\n",
      "30752.08421087265\n"
     ]
    }
   ],
   "source": [
    "# seed 15\n",
    "s = time()\n",
    "model_max, loss_c, loss_r, loss_p = main_dde_nn(25)\n",
    "e = time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
